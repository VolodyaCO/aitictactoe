{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "    \"\"\"\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [1, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]]\n",
    "\n",
    "    def shift(x, w):\n",
    "        return convolve(x.reshape((8, 8)), mode='constant', weights=w).ravel()\n",
    "\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "digits = datasets.load_digits()\n",
    "X = np.asarray(digits.data, 'float32')\n",
    "X, Y = nudge_dataset(X, [(yi, yi) for yi in digits.target])\n",
    "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models we will use\n",
    "logistic = MultiOutputRegressor(linear_model.LogisticRegression(solver='lbfgs', max_iter=10000,\n",
    "                                           multi_class='multinomial'))\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "rbm_features_classifier = Pipeline(\n",
    "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "\n",
    "# #############################################################################\n",
    "# Training\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 20\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -25.39, time = 1.13s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -23.77, time = 1.49s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -22.94, time = 1.36s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -21.91, time = 1.29s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -21.69, time = 1.37s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -21.06, time = 1.35s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -20.89, time = 1.41s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -20.64, time = 1.37s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -20.36, time = 1.44s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -20.09, time = 1.31s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -20.08, time = 1.32s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -19.82, time = 1.31s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -19.64, time = 0.87s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -19.61, time = 0.54s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -19.57, time = 0.76s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -19.41, time = 0.85s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -19.30, time = 0.77s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -19.25, time = 0.77s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -19.27, time = 0.80s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -19.01, time = 0.74s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('rbm', BernoulliRBM(batch_size=10, learning_rate=0.06, n_components=100, n_iter=20,\n",
       "       random_state=0, verbose=True)), ('logistic', MultiOutputRegressor(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training RBM-Logistic Pipeline\n",
    "rbm_features_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = rbm_features_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99       174\n",
      "          1       0.81      0.85      0.83       184\n",
      "          2       0.88      0.92      0.90       166\n",
      "          3       0.89      0.80      0.84       194\n",
      "          4       0.96      0.93      0.94       186\n",
      "          5       0.87      0.83      0.85       181\n",
      "          6       0.98      0.96      0.97       207\n",
      "          7       0.88      0.97      0.93       154\n",
      "          8       0.77      0.81      0.79       182\n",
      "          9       0.81      0.79      0.80       169\n",
      "\n",
      "avg / total       0.89      0.88      0.88      1797\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(Y_test[:,0], Y_pred[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputRegressor(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "           n_jobs=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Logistic regression classifier directly on the pixel\n",
    "raw_pixel_classifier = clone(logistic)\n",
    "raw_pixel_classifier.C = 100.\n",
    "raw_pixel_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.94      0.91       174\n",
      "          1       0.57      0.56      0.57       184\n",
      "          2       0.78      0.85      0.81       166\n",
      "          3       0.78      0.76      0.77       194\n",
      "          4       0.81      0.84      0.82       186\n",
      "          5       0.76      0.73      0.75       181\n",
      "          6       0.90      0.88      0.89       207\n",
      "          7       0.83      0.89      0.86       154\n",
      "          8       0.72      0.57      0.64       182\n",
      "          9       0.72      0.76      0.74       169\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1797\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = raw_pixel_classifier.predict(X_test)\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(Y_test[:,0], Y_pred[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
