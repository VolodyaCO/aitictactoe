{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from pytorch_classification.utils import Bar, AverageMeter\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board will be represented by a 3x3 matrix, with components -1, 0 or 1. -1(1) corresponds to player -1(1), and 0 to an empty square.\n",
    "\n",
    "A state is the same as the board, with a main difference: 1(-1) corresponds to the mark of the current(opponent) player.\n",
    "\n",
    "A play is a 3x3 matrix with all zeros except one component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    \"\"\"\n",
    "    This class specifies the base Game class. To define your own game, subclass\n",
    "    this class and implement the functions below. This works when the game is\n",
    "    two-player, adversarial and turn-based.\n",
    "    Use 1 for player1 and -1 for player2.\n",
    "    See othello/OthelloGame.py for an example implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros(3,3)\n",
    "\n",
    "    def getInitBoard(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            startBoard: a representation of the board (ideally this is the form\n",
    "                        that will be the input to your neural network)\n",
    "        \"\"\"\n",
    "        startBoard = self.board.flatten()\n",
    "        return startBoard\n",
    "\n",
    "    def getBoardSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (x,y): a tuple of board dimensions\n",
    "        \"\"\"\n",
    "        return (3,3)\n",
    "\n",
    "    def getActionSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            actionSize: number of all possible actions\n",
    "        \"\"\"\n",
    "        actionSize = 9\n",
    "        return actionSize\n",
    "\n",
    "    def getNextState(self, board, player, action):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "            action: action taken by current player\n",
    "        Returns:\n",
    "            nextBoard: board after applying action\n",
    "            nextPlayer: player who plays in the next turn (should be -player)\n",
    "        \"\"\"\n",
    "        play = np.zeros(9)\n",
    "        play[action] = player\n",
    "        nextBoard\n",
    "        return board + play, -player\n",
    "\n",
    "    def getValidMoves(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player\n",
    "        Returns:\n",
    "            validMoves: a binary vector of length self.getActionSize(), 1 for\n",
    "                        moves that are valid from the current board and player,\n",
    "                        0 for invalid moves\n",
    "        \"\"\"\n",
    "        validMoves = np.array([1 if boardSpot==0 else 0 for boardSpot in board])\n",
    "        return validMoves\n",
    "\n",
    "    def getGameEnded(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "        Returns:\n",
    "            r: 0 if game has not ended. 1 if player won, -1 if player lost,\n",
    "               small non-zero value for draw.\n",
    "               \n",
    "        \"\"\"\n",
    "        matrix = board.reshape(3,3)\n",
    "        w = 3*player\n",
    "        if np.any(matrix.sum(axis=0) == w) or np.any(np.trace(matrix) == w) or np.any(np.trace(np.fliplr(matrix)) == w):\n",
    "            # current player  wins\n",
    "            r = player\n",
    "        elif np.any(matrix.sum(axis=0) == -w) or np.any(np.trace(matrix) == -w) or np.any(np.trace(np.fliplr(matrix)) == -w):\n",
    "            # opponent player wins\n",
    "            r = -player\n",
    "        elif np.any(board != 0):\n",
    "            # game still ongoing\n",
    "            r = 0\n",
    "        else:\n",
    "            # game is a draw\n",
    "            r = 1e-6\n",
    "        return r\n",
    "\n",
    "    def getCanonicalForm(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "        Returns:\n",
    "            canonicalBoard: returns canonical form of board. The canonical form\n",
    "                            should be independent of player. For e.g. in chess,\n",
    "                            the canonical form can be chosen to be from the pov\n",
    "                            of white. When the player is white, we can return\n",
    "                            board as is. When the player is black, we can invert\n",
    "                            the colors and return the board.\n",
    "        \"\"\"\n",
    "        canonicalBoard = board*player\n",
    "        return canonicalBoard\n",
    "\n",
    "    def getSymmetries(self, board, pi):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            pi: policy vector of size self.getActionSize()\n",
    "        Returns:\n",
    "            symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
    "                       form of the board and the corresponding pi vector. This\n",
    "                       is used when training the neural network from examples.\n",
    "        \"\"\"\n",
    "        matrix = board.reshape(3,3)\n",
    "        pim = pi.reshape(3,3)\n",
    "        l=[]\n",
    "        for i in range(4):\n",
    "            rotm = np.rot90(matrix, i)\n",
    "            rotpi = np.rot90(pim, i)\n",
    "            l.append((np.flatten(rotm), np.flatten(rotpi)))\n",
    "            l.append((np.flatten(np.fliplr(rotm)), np.flatten(np.fliplr(rotpi))))\n",
    "        symmForms = list(set(l))\n",
    "        return symmForms\n",
    "\n",
    "    def stringRepresentation(self, board):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "        Returns:\n",
    "            boardString: a quick conversion of board to a string format.\n",
    "                         Required by MCTS for hashing.\n",
    "        \"\"\"\n",
    "        boardString = ''.join([str(x) for x in board])\n",
    "        return boardString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo part follows this convention:\n",
    "\n",
    "The Deep Boltzmann Machine (DBM)  takes states of the board $s$, by doing $f_\\theta:s\\mapsto (v_\\theta(s), \\vec{p}_\\theta(s))$, and outputs the board evaluation $v_\\theta(s)\\in[-1,1]$ (this may need to be adjusted in the DBM class that Juan Florez is writing), and a move policy $\\vec{p}_\\theta(s)$.\n",
    "\n",
    "When training the DBM, for each game we give it data of the form $(s_t, \\vec{\\pi}_t, z_t)$ for all states $s_t$ indexed by $t$. $\\vec{\\pi}_t$ is an estimate of the move policy from state $s_t$, and $z_t=-1,0,1$ is the outcome of the game, as seen by the player to play at time $t$. Therefore the DBM loss function is\n",
    "$$\n",
    "l = \\sum_t \\left[\\left(v_\\theta(s_t)-z_t\\right)^2-\\vec{\\pi}_t\\cdot \\log(\\vec{p}_\\theta(s_t))\\right]\n",
    "$$\n",
    "\n",
    "Let $Q(s,a)$ be the expected reward for making play $a$ from state $s$; $N(s,a)$ the number of times $a$ was played from $s$ across all simulations; $P(s,a)$ the probability that $a$ is played from $s$ according to the DBM. Therefore, the confidence upper bound is\n",
    "\n",
    "$$\n",
    "U(s,a) = Q(s,a) + c P(s,a) \\frac{\\sqrt{\\sum_b N(s,b)}}{N(s,a)+1},\n",
    "$$\n",
    "\n",
    "where $c$ is a constant that tunes the degree of exploration within the tree of moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores #times edge s,a was visited\n",
    "        self.Ns = {}        # stores #times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}        # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "        if temp==0:\n",
    "            bestA = np.argmax(counts)\n",
    "            probs = [0]*len(counts)\n",
    "            probs[bestA]=1\n",
    "            return probs\n",
    "\n",
    "        counts = [x**(1./temp) for x in counts]\n",
    "        probs = [x/float(sum(counts)) for x in counts]\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propogated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propogated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s]!=0:\n",
    "            # terminal node\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "                \n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, do workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s,a) in self.Qsa:\n",
    "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
    "            self.Nsa[(s,a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning. It uses the functions defined\n",
    "    in Game and NeuralNet. args are specified in main.py.\n",
    "    \"\"\"\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
    "        self.trainExamplesHistory = []    # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
    "        self.skipFirstSelfPlay = False # can be overriden in loadTrainExamples()\n",
    "\n",
    "    def executeEpisode(self):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1.\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
    "        uses temp=0.\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard,pi,v)\n",
    "                           pi is the MCTS informed policy vector, v is +1 if\n",
    "                           the player eventually won the game, else -1.\n",
    "        \"\"\"\n",
    "        trainExamples = []\n",
    "        board = self.game.getInitBoard()\n",
    "        self.curPlayer = 1\n",
    "        episodeStep = 0\n",
    "\n",
    "        while True:\n",
    "            episodeStep += 1\n",
    "            canonicalBoard = self.game.getCanonicalForm(board,self.curPlayer)\n",
    "            temp = int(episodeStep < self.args.tempThreshold)\n",
    "\n",
    "            pi = self.mcts.getActionProb(canonicalBoard, temp=temp)\n",
    "            sym = self.game.getSymmetries(canonicalBoard, pi)\n",
    "            for b,p in sym:\n",
    "                trainExamples.append([b, self.curPlayer, p, None])\n",
    "\n",
    "            action = np.random.choice(len(pi), p=pi)\n",
    "            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)\n",
    "\n",
    "            r = self.game.getGameEnded(board, self.curPlayer)\n",
    "\n",
    "            if r!=0:\n",
    "                return [(x[0],x[2],r*((-1)**(x[1]!=self.curPlayer))) for x in trainExamples]\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximium length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(1, self.args.numIters+1):\n",
    "            # bookkeeping\n",
    "            print('------ITER ' + str(i) + '------')\n",
    "            # examples of the iteration\n",
    "            if not self.skipFirstSelfPlay or i>1:\n",
    "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
    "    \n",
    "                eps_time = AverageMeter()\n",
    "                bar = Bar('Self Play', max=self.args.numEps)\n",
    "                end = time.time()\n",
    "    \n",
    "                for eps in range(self.args.numEps):\n",
    "                    self.mcts = MCTS(self.game, self.nnet, self.args)   # reset search tree\n",
    "                    iterationTrainExamples += self.executeEpisode()\n",
    "    \n",
    "                    # bookkeeping + plot progress\n",
    "                    eps_time.update(time.time() - end)\n",
    "                    end = time.time()\n",
    "                    bar.suffix  = '({eps}/{maxeps}) Eps Time: {et:.3f}s | Total: {total:} | ETA: {eta:}'.format(eps=eps+1, maxeps=self.args.numEps, et=eps_time.avg,\n",
    "                                                                                                               total=bar.elapsed_td, eta=bar.eta_td)\n",
    "                    bar.next()\n",
    "                bar.finish()\n",
    "\n",
    "                # save the iteration examples to the history \n",
    "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
    "                \n",
    "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
    "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
    "                self.trainExamplesHistory.pop(0)\n",
    "            # backup history to a file\n",
    "            # NB! the examples were collected using the model from the previous iteration, so (i-1)  \n",
    "            self.saveTrainExamples(i-1)\n",
    "            \n",
    "            # shuffle examlpes before training\n",
    "            trainExamples = []\n",
    "            for e in self.trainExamplesHistory:\n",
    "                trainExamples.extend(e)\n",
    "            shuffle(trainExamples)\n",
    "\n",
    "            # training new network, keeping a copy of the old one\n",
    "            self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            self.pnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            pmcts = MCTS(self.game, self.pnet, self.args)\n",
    "            \n",
    "            self.nnet.train(trainExamples)\n",
    "            nmcts = MCTS(self.game, self.nnet, self.args)\n",
    "\n",
    "            print('PITTING AGAINST PREVIOUS VERSION')\n",
    "            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),\n",
    "                          lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)\n",
    "            pwins, nwins, draws = arena.playGames(self.args.arenaCompare)\n",
    "\n",
    "            print('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
    "            if pwins+nwins > 0 and float(nwins)/(pwins+nwins) < self.args.updateThreshold:\n",
    "                print('REJECTING NEW MODEL')\n",
    "                self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            else:\n",
    "                print('ACCEPTING NEW MODEL')\n",
    "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))\n",
    "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')                \n",
    "\n",
    "    def getCheckpointFile(self, iteration):\n",
    "        return 'checkpoint_' + str(iteration) + '.pth.tar'\n",
    "\n",
    "    def saveTrainExamples(self, iteration):\n",
    "        folder = self.args.checkpoint\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        filename = os.path.join(folder, self.getCheckpointFile(iteration)+\".examples\")\n",
    "        with open(filename, \"wb+\") as f:\n",
    "            Pickler(f).dump(self.trainExamplesHistory)\n",
    "        f.closed\n",
    "\n",
    "    def loadTrainExamples(self):\n",
    "        modelFile = os.path.join(self.args.load_folder_file[0], self.args.load_folder_file[1])\n",
    "        examplesFile = modelFile+\".examples\"\n",
    "        if not os.path.isfile(examplesFile):\n",
    "            print(examplesFile)\n",
    "            r = input(\"File with trainExamples not found. Continue? [y|n]\")\n",
    "            if r != \"y\":\n",
    "                sys.exit()\n",
    "        else:\n",
    "            print(\"File with trainExamples found. Read it.\")\n",
    "            with open(examplesFile, \"rb\") as f:\n",
    "                self.trainExamplesHistory = Unpickler(f).load()\n",
    "            f.closed\n",
    "            # examples based on the model were already collected (loaded)\n",
    "            self.skipFirstSelfPlay = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arena():\n",
    "    \"\"\"\n",
    "    An Arena class where any 2 agents can be pit against each other.\n",
    "    \"\"\"\n",
    "    def __init__(self, player1, player2, game, display=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            player 1,2: two functions that takes board as input, return action\n",
    "            game: Game object\n",
    "            display: a function that takes board as input and prints it (e.g.\n",
    "                     display in othello/OthelloGame). Is necessary for verbose\n",
    "                     mode.\n",
    "        see othello/OthelloPlayers.py for an example. See pit.py for pitting\n",
    "        human players/other baselines with each other.\n",
    "        \"\"\"\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.game = game\n",
    "        self.display = display\n",
    "\n",
    "    def playGame(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Executes one episode of a game.\n",
    "        Returns:\n",
    "            either\n",
    "                winner: player who won the game (1 if player1, -1 if player2)\n",
    "            or\n",
    "                draw result returned from the game that is neither 1, -1, nor 0.\n",
    "        \"\"\"\n",
    "        players = [self.player2, None, self.player1]\n",
    "        curPlayer = 1\n",
    "        board = self.game.getInitBoard()\n",
    "        it = 0\n",
    "        while self.game.getGameEnded(board, curPlayer)==0:\n",
    "            it+=1\n",
    "            if verbose:\n",
    "                assert(self.display)\n",
    "                print(\"Turn \", str(it), \"Player \", str(curPlayer))\n",
    "                self.display(board)\n",
    "            action = players[curPlayer+1](self.game.getCanonicalForm(board, curPlayer))\n",
    "\n",
    "            valids = self.game.getValidMoves(self.game.getCanonicalForm(board, curPlayer),1)\n",
    "\n",
    "            if valids[action]==0:\n",
    "                print(action)\n",
    "                assert valids[action] >0\n",
    "            board, curPlayer = self.game.getNextState(board, curPlayer, action)\n",
    "        if verbose:\n",
    "            assert(self.display)\n",
    "            print(\"Game over: Turn \", str(it), \"Result \", str(self.game.getGameEnded(board, 1)))\n",
    "            self.display(board)\n",
    "        return self.game.getGameEnded(board, 1)\n",
    "\n",
    "    def playGames(self, num, verbose=False):\n",
    "        \"\"\"\n",
    "        Plays num games in which player1 starts num/2 games and player2 starts\n",
    "        num/2 games.\n",
    "        Returns:\n",
    "            oneWon: games won by player1\n",
    "            twoWon: games won by player2\n",
    "            draws:  games won by nobody\n",
    "        \"\"\"\n",
    "        eps_time = AverageMeter()\n",
    "        bar = Bar('Arena.playGames', max=num)\n",
    "        end = time.time()\n",
    "        eps = 0\n",
    "        maxeps = int(num)\n",
    "\n",
    "        num = int(num/2)\n",
    "        oneWon = 0\n",
    "        twoWon = 0\n",
    "        draws = 0\n",
    "        for _ in range(num):\n",
    "            gameResult = self.playGame(verbose=verbose)\n",
    "            if gameResult==1:\n",
    "                oneWon+=1\n",
    "            elif gameResult==-1:\n",
    "                twoWon+=1\n",
    "            else:\n",
    "                draws+=1\n",
    "            # bookkeeping + plot progress\n",
    "            eps += 1\n",
    "            eps_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            bar.suffix  = '({eps}/{maxeps}) Eps Time: {et:.3f}s | Total: {total:} | ETA: {eta:}'.format(eps=eps+1, maxeps=maxeps, et=eps_time.avg,\n",
    "                                                                                                       total=bar.elapsed_td, eta=bar.eta_td)\n",
    "            bar.next()\n",
    "\n",
    "        self.player1, self.player2 = self.player2, self.player1\n",
    "        \n",
    "        for _ in range(num):\n",
    "            gameResult = self.playGame(verbose=verbose)\n",
    "            if gameResult==-1:\n",
    "                oneWon+=1                \n",
    "            elif gameResult==1:\n",
    "                twoWon+=1\n",
    "            else:\n",
    "                draws+=1\n",
    "            # bookkeeping + plot progress\n",
    "            eps += 1\n",
    "            eps_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            bar.suffix  = '({eps}/{maxeps}) Eps Time: {et:.3f}s | Total: {total:} | ETA: {eta:}'.format(eps=eps+1, maxeps=num, et=eps_time.avg,\n",
    "                                                                                                       total=bar.elapsed_td, eta=bar.eta_td)\n",
    "            bar.next()\n",
    "            \n",
    "        bar.finish()\n",
    "\n",
    "        return oneWon, twoWon, draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juan must do this!\n",
    "class NeuralNet():\n",
    "    \"\"\"\n",
    "    This class specifies the base NeuralNet class. To define your own neural\n",
    "    network, subclass this class and implement the functions below. The neural\n",
    "    network does not consider the current player, and instead only deals with\n",
    "    the canonical form of the board.\n",
    "    See othello/NNet.py for an example implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game):\n",
    "        pass\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        This function trains the neural network with examples obtained from\n",
    "        self-play.\n",
    "        Input:\n",
    "            examples: a list of training examples, where each example is of form\n",
    "                      (board, pi, v). pi is the MCTS informed policy vector for\n",
    "                      the given board, and v is its value. The examples has\n",
    "                      board in its canonical form.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, board):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board in its canonical form.\n",
    "        Returns:\n",
    "            pi: a policy vector for the current board- a numpy array of length\n",
    "                game.getActionSize\n",
    "            v: a float in [-1,1] that gives the value of the current board\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_checkpoint(self, folder, filename):\n",
    "        \"\"\"\n",
    "        Saves the current neural network (with its parameters) in\n",
    "        folder/filename\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def load_checkpoint(self, folder, filename):\n",
    "        \"\"\"\n",
    "        Loads parameters of the neural network from folder/filename\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
