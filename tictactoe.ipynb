{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board will be represented by a 3x3 matrix, with components -1, 0 or 1. -1(1) corresponds to player -1(1), and 0 to an empty square. The board shall be saved in the self.board at all times.\n",
    "\n",
    "A state is the same as the board, with a main difference: -1(1) corresponds to the mark of the current(opponent) player.\n",
    "\n",
    "A play is a 3x3 matrix with all zeros except one component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board(object):\n",
    "    def start(self):\n",
    "        self.board = np.zeros(3, 3)\n",
    "        return self.board.flatten()\n",
    "\n",
    "    def current_player(self, state):\n",
    "        \"\"\"\n",
    "        Gets the current player number ::cplayNum::\n",
    "        \"\"\"\n",
    "        whos_turn = np.sum(np.abs(state))\n",
    "        if whos_turn%2 == 0:\n",
    "            cplayNum = -1\n",
    "        else:\n",
    "            cplayNum = 1\n",
    "        return cplayNum\n",
    "\n",
    "    def next_state(self, state, play):\n",
    "        \"\"\"\n",
    "        Takes the game state, and the move to be applied, returns the new game state.\n",
    "        \"\"\"\n",
    "        new_state = state + play\n",
    "        return new_state\n",
    "\n",
    "    def legal_plays(self, state):\n",
    "        \"\"\"\n",
    "        Takes the game state and returns the possible legal plays\n",
    "        \"\"\"\n",
    "        idx = np.where(state == 0)[0]\n",
    "        empty_play = np.zeros(9)\n",
    "        new_plays = []\n",
    "        for i in idx:\n",
    "            copy = empty_play[:]\n",
    "            copy[i] = -1\n",
    "            new_plays.append(copy)\n",
    "        return new_plays\n",
    "\n",
    "    def winner(self, state):\n",
    "        matrix = state.reshape(3,3)\n",
    "        if np.any(matrix.sum(axis=0) == 3) or np.any(np.trace(matrix) == 3) or np.any(np.trace(np.fliplr(matrix)) == 3):\n",
    "            # player 1 wins\n",
    "            winner = 1\n",
    "        elif np.any(matrix.sum(axis=0) == -3) or np.any(np.trace(matrix) == -3) or np.any(np.trace(np.fliplr(matrix)) == -3):\n",
    "            # player -1 wins\n",
    "            winner = -1\n",
    "        elif np.where(state == 0)[0]:\n",
    "            # game still ongoing\n",
    "            winner = 0\n",
    "        else:\n",
    "            # game is a draw\n",
    "            winner = 2\n",
    "        return winner\n",
    "    \n",
    "    def stringRepresentation(self, state):\n",
    "        return ''.join([str(x) for x in state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo part follows this convention:\n",
    "\n",
    "The Deep Boltzmann Machine (DBM)  takes states of the board $s$, by doing $f_\\theta:s\\mapsto (v_\\theta(s), \\vec{p}_\\theta(s))$, and outputs the board evaluation $v_\\theta(s)\\in[-1,1]$ (this may need to be adjusted in the DBM class that Juan Florez is writing), and a move policy $\\vec{p}_\\theta(s)$.\n",
    "\n",
    "When training the DBM, for each game we give it data of the form $(s_t, \\vec{\\pi}_t, z_t)$ for all states $s_t$ indexed by $t$. $\\vec{\\pi}_t$ is an estimate of the move policy from state $s_t$, and $z_t=-1,0,1$ is the outcome of the game, as seen by the player to play at time $t$. Therefore the DBM loss function is\n",
    "$$\n",
    "l = \\sum_t \\left[\\left(v_\\theta(s_t)-z_t\\right)^2-\\vec{\\pi}_t\\cdot \\log(\\vec{p}_\\theta(s_t))\\right]\n",
    "$$\n",
    "\n",
    "Let $Q(s,a)$ be the expected reward for making play $a$ from state $s$; $N(s,a)$ the number of times $a$ was played from $s$ across all simulations; $P(s,a)$ the probability that $a$ is played from $s$ according to the DBM. Therefore, the confidence upper bound is\n",
    "\n",
    "$$\n",
    "U(s,a) = Q(s,a) + c P(s,a) \\frac{\\sqrt{\\sum_b N(s,b)}}{N(s,a)+1},\n",
    "$$\n",
    "\n",
    "where $c$ is a constant that tunes the degree of exploration within the tree of moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-416b416ae87f>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-416b416ae87f>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    if temp==0:\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "EPS = 1e-8\n",
    "\n",
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, board, nnet, num_sims):\n",
    "        self.board = board\n",
    "        self.nnet = nnet\n",
    "        self.num_sims = num_sims\n",
    "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores #times edge s,a was visited\n",
    "        self.Ns = {}        # stores #times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}        # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, state, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs num_sims simulations of MCTS starting from\n",
    "        state.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.num_sims):\n",
    "            self.search(state)\n",
    "\n",
    "        s = self.board.stringRepresentation(state)\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in \n",
    "                  [self.board.stringRepresentation(x) for x in self.game.legal_plays(state)]]\n",
    "\n",
    "        if temp==0:\n",
    "            bestA = np.argmax(counts)\n",
    "            probs = [0]*len(counts)\n",
    "            probs[bestA]=1\n",
    "            return probs\n",
    "\n",
    "        counts = [x**(1./temp) for x in counts]\n",
    "        probs = [x/float(sum(counts)) for x in counts]\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def search(self, state):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propogated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propogated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current state\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.game.stringRepresentation(state)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(state, 1)\n",
    "        if self.Es[s]!=0:\n",
    "            # terminal node\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(state)\n",
    "            valids = self.game.getValidMoves(state, 1)\n",
    "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "                \n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, do workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s,a) in self.Qsa:\n",
    "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(state, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
    "            self.Nsa[(s,a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
